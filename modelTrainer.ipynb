{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "num_epochs = 50\n",
    "device = 'cpu'\n",
    "modelName = 'model.pth'\n",
    "historyFileName = 'history.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRNN model architecture\n",
    "# The model uses 3 CNN layers and 2 LSTM layers\n",
    "\n",
    "class MusicGenreCRNN(nn.Module):\n",
    "    def __init__(self, num_genres, input_channels):\n",
    "        super(MusicGenreCRNN, self).__init__()\n",
    "        \n",
    "        # CNN Layers -used for feature extraction from spectrogram\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "        )\n",
    "        \n",
    "        self.rnn_input_size = 256 * 16  \n",
    "        \n",
    "        # RNN layers - used for temporal analysis of features\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.rnn_input_size,\n",
    "            hidden_size=128,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(256, 64), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_genres)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = x.reshape(batch_size, -1, self.rnn_input_size)\n",
    "        \n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        \n",
    "        attention_weights = self.attention(rnn_out)\n",
    "        x = torch.sum(attention_weights * rnn_out, dim=1)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We trained the model on our own laptop, so we had to use CPU, since we don't have a dedicated GPU\n",
    "# If you however have a GPU, set devide='cuda'. GPU is generally faster than CPU\n",
    "# Adjust the num_epochs as well if you want. More epochs = longer training time, but might provide better results\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device):\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'epoch': []\n",
    "    }\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "\n",
    "    # Training phase - goes through each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device).squeeze()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}, '\n",
    "                      f'Acc: {100.*correct/total:.2f}%')\n",
    "        \n",
    "        epoch_train_loss = train_loss/len(train_loader)\n",
    "        epoch_train_acc = 100.*correct/total\n",
    "        \n",
    "        # Validation phase \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device).squeeze()\n",
    "                outputs, _ = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        epoch_val_loss = val_loss/len(val_loader)\n",
    "        epoch_val_acc = 100.*val_correct/val_total\n",
    "        \n",
    "        # Stores the training and validation metrics in history dictionary - very useful for plotting later\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "        history['epoch'].append(epoch)\n",
    "        \n",
    "        # Prints metrics for each epoch - lets you follow the training in real time\n",
    "        print(f'\\nEpoch: {epoch}')\n",
    "        print(f'Training Loss: {epoch_train_loss:.4f}')\n",
    "        print(f'Training Accuracy: {epoch_train_acc:.2f}%')\n",
    "        print(f'Validation Loss: {epoch_val_loss:.4f}')\n",
    "        print(f'Validation Accuracy: {epoch_val_acc:.2f}%\\n')\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'history': history,\n",
    "        }\n",
    "        torch.save(checkpoint, f'revised_checkpoint_epoch_{epoch}.pth')\n",
    "        \n",
    "        # Save the model\n",
    "        # IMPORTANT: Rename the model to whatever you want\n",
    "        # we used the following for simple overview: {num_epochs}ep{songDuration}s_model.pth\n",
    "        if epoch == 0 or epoch_val_loss < min(history['val_loss'][:-1]):\n",
    "            torch.save(checkpoint, modelName)\n",
    "    \n",
    "    # Save history to JSON file\n",
    "    # Again, rename it however you want\n",
    "    import json\n",
    "    with open(historyFileName, 'w') as f:\n",
    "        json.dump(history, f)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Function to load and plot metrics\n",
    "def plot_training_history(history_path=historyFileName):\n",
    "    with open(history_path, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    plt.style.use('seaborn')\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax1.plot(history['epoch'], history['train_acc'], label='Training Accuracy', color='#1f77b4')\n",
    "    ax1.plot(history['epoch'], history['val_acc'], label='Validation Accuracy', color='#ff7f0e')\n",
    "    ax1.set_title('Model Accuracy over Epochs', pad=20)\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(history['epoch'], history['train_loss'], label='Training Loss', color='#1f77b4')\n",
    "    ax2.plot(history['epoch'], history['val_loss'], label='Validation Loss', color='#ff7f0e')\n",
    "    ax2.set_title('Model Loss over Epochs', pad=20)\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# There are also options to implement continuing trainnig the model from a checkpoint\n",
    "# but we haven't implemented it as there was no need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets the files for training and also testing\n",
    "def get_files_and_labels(base_path):\n",
    "    files = []\n",
    "    labels = []\n",
    "    \n",
    "    genres = sorted(os.listdir(base_path))\n",
    "    \n",
    "    # Create genre to label mapping (e.g., 'blues': 0, 'classical': 1, etc.)\n",
    "    # This is useful for converting genre names to integers for training\n",
    "    genre_to_label = {genre: idx for idx, genre in enumerate(genres)}\n",
    "    \n",
    "    for genre in genres:\n",
    "        genre_path = os.path.join(base_path, genre)\n",
    "        if os.path.isdir(genre_path):\n",
    "            for file in os.listdir(genre_path):\n",
    "                if file.endswith(('.wav')):\n",
    "                    files.append(os.path.join(genre_path, file))\n",
    "                    labels.append(genre_to_label[genre])\n",
    "    \n",
    "    return files, labels, genre_to_label\n",
    "\n",
    "\n",
    "def prepare_dataset():\n",
    "    # Paths to training, vlaidation and test data folders\n",
    "    train_path = \"./data/train_files\"  \n",
    "    val_path = \"./data/validation_files\"      \n",
    "    test_path = \"./data/test_files\"    \n",
    "    \n",
    "    train_files, train_labels, genre_mapping = get_files_and_labels(train_path)\n",
    "    val_files, val_labels, _ = get_files_and_labels(val_path)\n",
    "    test_files, test_labels, _ = get_files_and_labels(test_path)\n",
    "    \n",
    "    print(\"\\nDataset statistics:\")\n",
    "    print(f\"Training files: {len(train_files)}\")\n",
    "    print(f\"Validation files: {len(val_files)}\")\n",
    "    print(f\"Test files: {len(test_files)}\")\n",
    "    \n",
    "    # Shows the distribution of genres and files\n",
    "    print(\"\\nTraining set genre distribution:\")\n",
    "    unique_labels, counts = np.unique(train_labels, return_counts=True)\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        genre = list(genre_mapping.keys())[list(genre_mapping.values()).index(label)]\n",
    "        print(f\"{genre}: {count} files\")\n",
    "    \n",
    "    return (train_files, train_labels), (val_files, val_labels), (test_files, test_labels), genre_mapping\n",
    "\n",
    "(train_files, train_labels), (val_files, val_labels), (test_files, test_labels), genre_mapping = prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses Librosa to load the music files and convert them to mel spectrograms\n",
    "# We also add some random augmentations to the data to make the model more robust\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, audio_files, labels):\n",
    "        self.audio_files = audio_files\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            audio, sr = librosa.load(self.audio_files[idx], duration=15, sr=22050)\n",
    "\n",
    "            # Random augmentations\n",
    "            if np.random.random() > 0.5:\n",
    "                rate = np.random.uniform(0.8, 1.2)\n",
    "                audio = librosa.effects.time_stretch(audio, rate=rate)\n",
    "            \n",
    "            if np.random.random() > 0.5:\n",
    "                steps = np.random.randint(-2, 3)\n",
    "                audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=steps)\n",
    "                \n",
    "            # MEL spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=audio,\n",
    "                sr=sr,\n",
    "                n_mels=128,\n",
    "                n_fft=2048,\n",
    "                hop_length=512\n",
    "            )\n",
    "            \n",
    "            # Log scale\n",
    "            mel_spec = librosa.power_to_db(mel_spec)\n",
    "            \n",
    "            target_length = 1292\n",
    "            if mel_spec.shape[1] > target_length:\n",
    "                mel_spec = mel_spec[:, :target_length]\n",
    "            elif mel_spec.shape[1] < target_length:\n",
    "                pad_width = target_length - mel_spec.shape[1]\n",
    "                mel_spec = np.pad(mel_spec, ((0, 0), (0, pad_width)), mode='constant')\n",
    "            \n",
    "\n",
    "\n",
    "            mel_spec = (mel_spec - mel_spec.mean()) / (mel_spec.std() + 1e-8)\n",
    "            \n",
    "            # Converts to PyTorch tensor\n",
    "            mel_spec = torch.FloatTensor(mel_spec).unsqueeze(0)\n",
    "            label = torch.LongTensor([self.labels[idx]])\n",
    "            \n",
    "            return mel_spec, label\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing index {idx}, file {self.audio_files[idx]}: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main method, entrypoint for the script\n",
    "# Loads the training and validation datasets, creates loaders and initializes the model training\n",
    "\n",
    "def main():\n",
    "    train_dataset = MusicDataset(train_files, train_labels)\n",
    "    val_dataset = MusicDataset(val_files, val_labels)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=16,  # Reduced from 32\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=16,  # Reduced from 32\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initializs the model\n",
    "    model = MusicGenreCRNN(\n",
    "        num_genres=10,\n",
    "        input_channels=1\n",
    "    )\n",
    "    \n",
    "    # Load single batch before training to check if everything works\n",
    "    print(\"Testing data loader...\")\n",
    "    try:\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            print(f\"Successfully loaded batch {batch_idx}\")\n",
    "            if batch_idx == 0:  # Just test one batch\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading batch: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    history = train_model(model, train_loader, val_loader, num_epochs, device)\n",
    "    plot_training_history(historyFileName)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicgenre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
